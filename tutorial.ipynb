{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "batch_first = True 는, n grams? input vector 를 열로 만들것인지, 행으로 만들것인지를 결정하는 옵션\n",
    "fix_length = 해당 파일에서 최대 몇 단어까지 들고올 것 인지에 대한 설정값, 200 이하면 <pad>를 채워서 강제로 200 길이에 맞춤\n",
    "'''\n",
    "TEXT = torchtext.data.Field(lower=True, batch_first=False, fix_length=200)\n",
    "LABEL = torchtext.data.Field(sequential=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이렇게 splits로 들고오면, pos/neg 밖에 구분하지 못함. score 값은 적용이 안됨 현재는 1 : neg, 2 : pos\n",
    "'''\n",
    "train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TEXT의 vocab를 보면 실제로는 10000을 넘어가는데, 그 이유는 추가적으로 필요한 <unk>, <pad> ... 가 있기 때문이다.\n",
    "'''\n",
    "TEXT.build_vocab(train, vectors=torchtext.vocab.GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\n",
    "'''\n",
    "3개 있음, neg / pos / unk\n",
    "'''\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits((train, test), batch_size=32, device=device, shuffle=True)\n",
    "\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBRnn(torch.nn.Module):\n",
    "    '''\n",
    "    n_vocab : # vocab\n",
    "    hidden_size : # features in the hidden state h\n",
    "    n_cat : # outputs\n",
    "    bs : batch\n",
    "    nl : # layers\n",
    "    '''\n",
    "    def __init__(self, n_vocab, hidden_size, n_cat, bs=1, nl=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = bs\n",
    "        self.nl = nl\n",
    "        self.e = torch.nn.Embedding(n_vocab, hidden_size)\n",
    "        self.rnn = torch.nn.LSTM(hidden_size, hidden_size, nl)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, n_cat)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        bs = inp.size()[1]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        e_out = self.e(inp)\n",
    "        # For every batch step, h0 and c0 are should be initialized with zero\n",
    "        h0 = c0 = e_out.data.new(*(self.nl, self.bs, self.hidden_size)).zero_()\n",
    "        rnn_o, _ = self.rnn(e_out, (h0, c0))\n",
    "        # Real output of the network\n",
    "        rnn_o = rnn_o[-1]\n",
    "        fc = F.dropout(self.fc2(rnn_o), p=0.5)\n",
    "        return self.softmax(fc)\n",
    "    \n",
    "\n",
    "def fit(epoch, model, optimizer, data_loader, phase='training', volatile=False):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        text, target = batch.text, batch.label\n",
    "        if torch.cuda.is_available():\n",
    "            text, target = text.cuda(), target.cuda()\n",
    "            \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        running_loss += F.nll_loss(output, target, reduction='sum').data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct / len(data_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print(f'{phase} loss is {loss:5.2f} and {phase} accuracy is {running_correct} / {len(data_loader.dataset)} {accuracy:10.4f}')\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is  0.88 and training accuracy is 11090 / 25000    44.3600\n",
      "validation loss is  0.85 and validation accuracy is 11446 / 25000    45.7840\n",
      "training loss is  0.85 and training accuracy is 11588 / 25000    46.3520\n",
      "validation loss is  0.83 and validation accuracy is 12295 / 25000    49.1800\n",
      "training loss is  0.74 and training accuracy is 13487 / 25000    53.9480\n",
      "validation loss is  0.67 and validation accuracy is 14384 / 25000    57.5360\n",
      "training loss is  0.65 and training accuracy is 14526 / 25000    58.1040\n",
      "validation loss is  0.63 and validation accuracy is 14586 / 25000    58.3440\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(TEXT.vocab.stoi)\n",
    "n_hidden = 300\n",
    "\n",
    "model = IMDBRnn(n_vocab, n_hidden, 3, bs=32)\n",
    "\n",
    "model.e.weight.data = TEXT.vocab.vectors\n",
    "model.e.weight.requires_grad = False\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, optimizer, train_iter, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(epoch, model, optimizer, test_iter, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
